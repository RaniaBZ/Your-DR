{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U xformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U einops\n",
    "!pip install -q -U nvidia-ml-py3\n",
    "!pip install -q -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ZappY-AI/MASHQA-JSON\", split=\"train[:20%]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "  system_prompt_template = \"\"\"[INST]Act as a Multiple Answer Spans Healthcare Question Answering helpful assistant and answer the user's questions in details with reasoning. Do not give any false information. In case you don't have answer, specify why the question can't be answered.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "  user_message = sample['question']\n",
    "  user_response = sample['answer']\n",
    "  prompt_template = system_prompt_template.replace(\"<>\",f\"{user_message}\").replace(\"<>\",f\"{user_response}\")\n",
    "\n",
    "  return {\"inputs\":prompt_template}\n",
    "\n",
    "#\n",
    "instruct_tune_dataset = dataset.map(create_prompt)\n",
    "print(instruct_tune_dataset)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "instruct_tune_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from pynvml import *\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import time, torch\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "#Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_eos_token=True, use_fast=True, max_length=250)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\") #change to bfloat16 if are using an Ampere (or more recent) GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          base_model_id, trust_remote_code=True, quantization_config=bnb_config, revision=\"refs/pr/23\", device_map={\"\": 0}, torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True\n",
    ")\n",
    "print(print_gpu_utilization())\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"Wqkv\", \"out_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./mistral-MASHQA-results2\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=12,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=100,\n",
    "        logging_steps=25,\n",
    "        learning_rate=1e-4,\n",
    "        eval_steps=50,\n",
    "        optim='paged_adamw_8bit',\n",
    "        fp16=True, #change to bf16 if are using an Ampere GPU\n",
    "        num_train_epochs=3,\n",
    "        max_steps=500,\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = instruct_tune_dataset.map(batched=True,remove_columns=['answer', 'question'])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        #eval_dataset=dataset['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"inputs\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"fine-tuned adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Manually input the data from the provided metrics\n",
    "steps = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475]\n",
    "training_loss = [1.1485, 1.0147, 0.7141, 0.6297, 0.6046, 0.5911, 0.5822, 0.5772, 0.5741, 0.565, 0.5635, 0.5659, 0.5514, 0.5472, 0.561, 0.5643, 0.5606, 0.5521, 0.5567]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, training_loss, marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Mistral-MASHQA\\nQLoRA Training Loss Over Time', fontsize=16)\n",
    "plt.xlabel('Training Steps', fontsize=14)\n",
    "plt.ylabel('Training Loss', fontsize=14)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(steps, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['Training Loss'], fontsize=12)\n",
    "\n",
    "# Add annotation to indicate minimum loss\n",
    "min_loss_index = training_loss.index(min(training_loss))\n",
    "plt.annotate(f'Min Loss: {min(training_loss):.4f}', xy=(steps[min_loss_index], training_loss[min_loss_index]),\n",
    "             xytext=(steps[min_loss_index] + 25, training_loss[min_loss_index] + 0.05),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
